{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gdemil24\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emotion: anger, Number of Posts: 110\n",
      "        tweet_id sentiment           author  \\\n",
      "494   1957083641     anger         elDi_irk   \n",
      "527   1957089935     anger            umi78   \n",
      "612   1957110088     anger        NayNay_Rt   \n",
      "1377  1957289252     anger       crazy_erin   \n",
      "1384  1957291305     anger  msfussybritches   \n",
      "\n",
      "                                                content  \n",
      "494                               fuckin'm transtelecom  \n",
      "527                    Working   But it's Fridaaaayyyyy  \n",
      "612                          Packing  I don't like it..  \n",
      "1377  I tried to dye my hair and all i got was a blo...  \n",
      "1384  &quot;locked up abroad&quot; makes bein half b...  \n",
      "\n",
      "Emotion: boredom, Number of Posts: 179\n",
      "       tweet_id sentiment           author  \\\n",
      "112  1956993007   boredom         villa_ld   \n",
      "316  1957038475   boredom          kameezy   \n",
      "345  1957044366   boredom       snoopync18   \n",
      "495  1957083786   boredom     munkeysmomma   \n",
      "583  1957102562   boredom  Quastbabynumbr5   \n",
      "\n",
      "                                               content  \n",
      "112                                       i'm so tired  \n",
      "316                            Waiting in line @ tryst  \n",
      "345             why did i agree to work a double shift  \n",
      "495  is really, really bored... I guess I will go t...  \n",
      "583                       So deep its priecing my soul  \n",
      "\n",
      "Emotion: empty, Number of Posts: 827\n",
      "       tweet_id sentiment      author  \\\n",
      "0    1956967341     empty  xoshayzers   \n",
      "44   1956978276     empty      Aggie9   \n",
      "52   1956979917     empty  Dreness023   \n",
      "141  1957000336     empty       Gen22   \n",
      "180  1957007357     empty       _SNO_   \n",
      "\n",
      "                                               content  \n",
      "0    @tiffanylue i know  i was listenin to bad habi...  \n",
      "44   @creyes middle school and elem. High schools w...  \n",
      "52             @djmicdamn hey yu lil fucker i textd yu  \n",
      "141  @softtouchme just answered you- never learned ...  \n",
      "180      Have a headache  I'm going to bed. Goodnight!  \n",
      "\n",
      "Emotion: enthusiasm, Number of Posts: 759\n",
      "       tweet_id   sentiment        author  \\\n",
      "3    1956967789  enthusiasm   czareaquino   \n",
      "56   1956981427  enthusiasm       Caillie   \n",
      "422  1957066701  enthusiasm     FinIsKing   \n",
      "424  1957067779  enthusiasm  Maureen12683   \n",
      "444  1957073668  enthusiasm       kort030   \n",
      "\n",
      "                                               content  \n",
      "3                 wants to hang out with friends SOON!  \n",
      "56   bed...sorta. today was good, sara has strep th...  \n",
      "422                                I want another tatt  \n",
      "424         So, I need to make a lot of money tomorrow  \n",
      "444  @lilxamyx08 i know ridiculous! we never got to...  \n",
      "\n",
      "Emotion: fun, Number of Posts: 1776\n",
      "       tweet_id sentiment         author  \\\n",
      "21   1956972097       fun  schiz0phren1c   \n",
      "41   1956977187       fun     diNGUYEN31   \n",
      "148  1957001854       fun       Samiijoo   \n",
      "165  1957005696       fun      nickhalme   \n",
      "179  1957007268       fun       EmaMolly   \n",
      "\n",
      "                                               content  \n",
      "21   Wondering why I'm awake at 7am,writing a new s...  \n",
      "41   @DavidArchie &lt;3 your gonna be the first  tw...  \n",
      "148   RIP leonardo. You were a great mini fiddler crab  \n",
      "165  @IdleThumbs Up is out?  I didn't get the memo ...  \n",
      "179  @relly1  OMG Ur alive!!! LOL  2day has gone so...  \n",
      "\n",
      "Emotion: happiness, Number of Posts: 5209\n",
      "       tweet_id  sentiment       author  \\\n",
      "40   1956977084  happiness     ktierson   \n",
      "69   1956983874  happiness         joia   \n",
      "77   1956985535  happiness   quarrygirl   \n",
      "126  1956996765  happiness  rarmendariz   \n",
      "233  1957017522  happiness   Davislb921   \n",
      "\n",
      "                                               content  \n",
      "40   mmm much better day... so far! it's still quit...  \n",
      "69   So great to see Oin &amp; Cynthia.  So happy. ...  \n",
      "77   @havingmysay  dude, that is my favorite sandwi...  \n",
      "126  Need to pack for CALI CALI! Cannot waittt! Thi...  \n",
      "233  took a math test today. The day before the tes...  \n",
      "\n",
      "Emotion: hate, Number of Posts: 1323\n",
      "       tweet_id sentiment       author  \\\n",
      "30   1956974706      hate  MavrickAces   \n",
      "86   1956987600      hate    naterkane   \n",
      "99   1956989601      hate     M0anique   \n",
      "101  1956990288      hate  MissPassion   \n",
      "105  1956991009      hate      rdyfrde   \n",
      "\n",
      "                                               content  \n",
      "30   It is so annoying when she starts typing on he...  \n",
      "86   dammit! hulu desktop has totally screwed up my...  \n",
      "99   @ cayogial i wanted to come to BZ this summer ...  \n",
      "101                @mrgenius23 You win ... SIGH Rakeem  \n",
      "105                      @soviet_star Damn, that sucks  \n",
      "\n",
      "Emotion: love, Number of Posts: 3842\n",
      "       tweet_id sentiment          author  \\\n",
      "16   1956971170      love    poppygallico   \n",
      "94   1956989093      love    Angela_Grace   \n",
      "122  1956996385      love          votech   \n",
      "131  1956998370      love   rockinchick11   \n",
      "151  1957002773      love  UncoolRockstar   \n",
      "\n",
      "                                               content  \n",
      "16                                @annarosekerr agreed  \n",
      "94   @RobertF3 correct! I ADORE him. I just plucked...  \n",
      "122  @freepbx sounds good. Appreciate the suggestio...  \n",
      "131  Pats in philly at 2 am. I love it. Mmm cheeses...  \n",
      "151  @NisforNeemah thanks neemah. I'm gonna be sooo...  \n",
      "\n",
      "Emotion: neutral, Number of Posts: 8638\n",
      "      tweet_id sentiment       author  \\\n",
      "4   1956968416   neutral    xkilljoyx   \n",
      "10  1956969456   neutral   feinyheiny   \n",
      "22  1956972116   neutral        jansc   \n",
      "31  1956975441   neutral  LovableKeKe   \n",
      "32  1956975860   neutral   analalalah   \n",
      "\n",
      "                                              content  \n",
      "4   @dannycastillo We want to trade with someone w...  \n",
      "10                                   cant fall asleep  \n",
      "22  No Topic Maps talks at the Balisage Markup Con...  \n",
      "31                          @cynthia_123 i cant sleep  \n",
      "32                    I missed the bl***y bus!!!!!!!!  \n",
      "\n",
      "Emotion: relief, Number of Posts: 1526\n",
      "       tweet_id sentiment          author  \\\n",
      "147  1957001506    relief      benmfowler   \n",
      "215  1957014389    relief     misscinders   \n",
      "312  1957037860    relief          oh_slc   \n",
      "380  1957053409    relief  gangstamittens   \n",
      "406  1957062621    relief        antonywu   \n",
      "\n",
      "                                               content  \n",
      "147                                        I'm at work  \n",
      "215  is done painting all the bedroom furniture, I ...  \n",
      "312  Scary lightning and thunder  I'm glad it's ove...  \n",
      "380                     is home.  safely... but hungry  \n",
      "406  just finished 8 hours of Texas Life Insurance ...  \n",
      "\n",
      "Emotion: sadness, Number of Posts: 5165\n",
      "     tweet_id sentiment       author  \\\n",
      "1  1956967666   sadness    wannamama   \n",
      "2  1956967696   sadness    coolfunky   \n",
      "6  1956968487   sadness     ShansBee   \n",
      "8  1956969035   sadness  nic0lepaula   \n",
      "9  1956969172   sadness   Ingenue_Em   \n",
      "\n",
      "                                             content  \n",
      "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
      "2                Funeral ceremony...gloomy friday...  \n",
      "6  I should be sleep, but im not! thinking about ...  \n",
      "8            @charviray Charlene my love. I miss you  \n",
      "9         @kelcouch I'm sorry  at least it's Friday?  \n",
      "\n",
      "Emotion: surprise, Number of Posts: 2187\n",
      "       tweet_id sentiment        author  \\\n",
      "14   1956970860  surprise  okiepeanut93   \n",
      "164  1957005410  surprise        HIM357   \n",
      "189  1957008434  surprise     Just_Cath   \n",
      "190  1957008478  surprise    bekahjayne   \n",
      "192  1957008766  surprise     shutitoff   \n",
      "\n",
      "                                               content  \n",
      "14                                        Got the news  \n",
      "164  2 days of this month left, and I only have 400...  \n",
      "189  @Bern_morley where are you? In Bris? I can't h...  \n",
      "190  bec vs fat food   --- winner = fat food  but n...  \n",
      "192  I had a dream about a pretty pretty beach and ...  \n",
      "\n",
      "Emotion: worry, Number of Posts: 8459\n",
      "      tweet_id sentiment         author  \\\n",
      "5   1956968477     worry  xxxPEACHESxxx   \n",
      "7   1956968636     worry       mcsleazy   \n",
      "11  1956969531     worry   dudeitsmanda   \n",
      "18  1956971473     worry          LCJ82   \n",
      "20  1956971981     worry  andreagauster   \n",
      "\n",
      "                                              content  \n",
      "5   Re-pinging @ghostridah14: why didn't you go to...  \n",
      "7                Hmmm. http://www.djhero.com/ is down  \n",
      "11                            Choked on her retainers  \n",
      "18  @PerezHilton lady gaga tweeted about not being...  \n",
      "20  @raaaaaaek oh too bad! I hope it gets better. ...  \n",
      "\n",
      "Sample data for 'sadness':\n",
      "     tweet_id sentiment       author  \\\n",
      "1  1956967666   sadness    wannamama   \n",
      "2  1956967696   sadness    coolfunky   \n",
      "6  1956968487   sadness     ShansBee   \n",
      "8  1956969035   sadness  nic0lepaula   \n",
      "9  1956969172   sadness   Ingenue_Em   \n",
      "\n",
      "                                             content  \n",
      "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
      "2                Funeral ceremony...gloomy friday...  \n",
      "6  I should be sleep, but im not! thinking about ...  \n",
      "8            @charviray Charlene my love. I miss you  \n",
      "9         @kelcouch I'm sorry  at least it's Friday?  \n",
      "\n",
      "\n",
      " Statistical Summary:\n",
      "            Vocabulary Size  Min Length  Max Length  Avg Length  \\\n",
      "anger                 801.0         1.0        42.0       17.53   \n",
      "boredom              1041.0         3.0        39.0       16.57   \n",
      "empty                3095.0         1.0       106.0       14.33   \n",
      "enthusiasm           3002.0         2.0        38.0       16.77   \n",
      "fun                  6098.0         2.0        45.0       18.15   \n",
      "happiness           11925.0         1.0        82.0       16.88   \n",
      "hate                 4629.0         2.0        54.0       17.73   \n",
      "love                 8842.0         1.0        74.0       16.76   \n",
      "neutral             17430.0         1.0        91.0       14.14   \n",
      "relief               4735.0         1.0        41.0       16.98   \n",
      "sadness             10916.0         1.0        46.0       17.02   \n",
      "surprise             6657.0         2.0        57.0       17.23   \n",
      "worry               16115.0         1.0        49.0       17.32   \n",
      "\n",
      "            Std Dev Length  Avg Pronouns  Avg Uncommon  Avg Repetitions  \n",
      "anger                 8.70          1.86          1.24             1.75  \n",
      "boredom               8.35          1.37          1.06             1.42  \n",
      "empty                 9.02          1.23          1.16             1.33  \n",
      "enthusiasm            8.31          1.53          1.19             1.53  \n",
      "fun                   8.16          1.55          1.35             1.79  \n",
      "happiness             8.42          1.43          1.24             1.67  \n",
      "hate                  8.62          1.76          1.06             1.84  \n",
      "love                  8.53          1.70          1.22             1.71  \n",
      "neutral               8.57          1.24          1.28             1.22  \n",
      "relief                8.30          1.56          1.09             1.50  \n",
      "sadness               8.51          1.76          1.03             1.60  \n",
      "surprise              8.46          1.64          1.32             1.77  \n",
      "worry                 8.43          1.80          1.10             1.60  \n"
     ]
    }
   ],
   "source": [
    "#question 1\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('text_emotion.csv')\n",
    "\n",
    "##########\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Group data by 'sentiment' and create a dictionary of DataFrames\n",
    "emotion_dataframes = {emotion: df_group for emotion, df_group in df.groupby('sentiment')}\n",
    "\n",
    "# Save each category DataFrame for verification\n",
    "for emotion, emotion_df in emotion_dataframes.items():\n",
    "    print(f\"\\nEmotion: {emotion}, Number of Posts: {len(emotion_df)}\")\n",
    "    print(emotion_df.head())  # Print the first few rows of each category\n",
    "\n",
    "# Example: Accessing a specific emotion dataframe\n",
    "sadness_df = emotion_dataframes.get('sadness')\n",
    "print(\"\\nSample data for 'sadness':\")\n",
    "print(sadness_df.head())\n",
    "###########\n",
    "\n",
    "# Define pronouns comprehensively (including possessive/reflexive)\n",
    "PRONOUNS = {\n",
    "    'i', 'me', 'my', 'mine', 'myself',\n",
    "    'you', 'your', 'yours', 'yourself', 'yourselves',\n",
    "    'he', 'him', 'his', 'himself',\n",
    "    'she', 'her', 'hers', 'herself',\n",
    "    'it', 'its', 'itself',\n",
    "    'we', 'us', 'our', 'ours', 'ourselves',\n",
    "    'they', 'them', 'their', 'theirs', 'themselves'\n",
    "}\n",
    "\n",
    "def calculate_stats(df):\n",
    "    posts = df['content'].dropna()  # Handle missing values\n",
    "    vocab = set()\n",
    "    token_lengths = []\n",
    "    total_pronouns = 0\n",
    "    total_uncommon = 0\n",
    "    total_repetitions = 0\n",
    "\n",
    "    # Precompile regex for efficiency\n",
    "    uncommon_pattern = re.compile(r'[@$%&*#]|\\d')  # Explicitly target uncommon chars\n",
    "    \n",
    "    for post in posts:\n",
    "        # Tokenize and normalize\n",
    "        tokens = word_tokenize(post.lower())\n",
    "        vocab.update(tokens)\n",
    "        token_lengths.append(len(tokens))\n",
    "        \n",
    "        # Count pronouns\n",
    "        total_pronouns += sum(1 for token in tokens if token in PRONOUNS)\n",
    "        \n",
    "        # Count uncommon characters\n",
    "        total_uncommon += len(uncommon_pattern.findall(post))\n",
    "        \n",
    "        # Count repetitions (using frequency distribution)\n",
    "        freq = Counter(tokens)\n",
    "        total_repetitions += sum(cnt - 1 for cnt in freq.values() if cnt > 1)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    return {\n",
    "        \"vocab_size\": len(vocab),\n",
    "        \"min_length\": np.min(token_lengths) if token_lengths else 0,\n",
    "        \"max_length\": np.max(token_lengths) if token_lengths else 0,\n",
    "        \"avg_length\": np.mean(token_lengths) if token_lengths else 0,\n",
    "        \"std_dev_length\": np.std(token_lengths) if token_lengths else 0,\n",
    "        \"avg_pronouns\": total_pronouns / len(posts) if posts.size > 0 else 0,\n",
    "        \"avg_uncommon\": total_uncommon / len(posts) if posts.size > 0 else 0,\n",
    "        \"avg_repetitions\": total_repetitions / len(posts) if posts.size > 0 else 0,\n",
    "    }\n",
    "\n",
    "# Group data and calculate stats\n",
    "emotion_dataframes = {emotion: group for emotion, group in df.groupby('sentiment')}\n",
    "stats_summary = {emotion: calculate_stats(group) for emotion, group in emotion_dataframes.items()}\n",
    "\n",
    "# Convert to DataFrame\n",
    "stats_df = pd.DataFrame(stats_summary).T\n",
    "stats_df.columns = [\n",
    "    \"Vocabulary Size\", \"Min Length\", \"Max Length\", \"Avg Length\",\n",
    "    \"Std Dev Length\", \"Avg Pronouns\", \"Avg Uncommon\", \"Avg Repetitions\"\n",
    "]\n",
    "\n",
    "print(\"\\n\\n Statistical Summary:\")\n",
    "print(stats_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data for 'sadness' saved for latex use:\n",
      "Statistics Summary saved for latex use:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the DataFrame as a LaTeX table\n",
    "sadness_df.to_latex(\"Sample_data_for_'sadness'.tex\", index=True)\n",
    "print(\"Sample data for 'sadness' saved for latex use:\")\n",
    "\n",
    "stats_df.to_latex(\"stats_summary.tex\", index=True, float_format=\"%.2f\")\n",
    "print(\"Statistics Summary saved for latex use:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary Overlap Matrix (Jaccard Similarity):\n",
      "            empty  sadness  enthusiasm  neutral  worry  surprise   love  \\\n",
      "empty       1.000    0.140       0.202    0.103  0.109     0.172  0.148   \n",
      "sadness     0.140    1.000       0.140    0.197  0.219     0.205  0.205   \n",
      "enthusiasm  0.202    0.140       1.000    0.103  0.109     0.175  0.157   \n",
      "neutral     0.103    0.197       0.103    1.000  0.212     0.165  0.180   \n",
      "worry       0.109    0.219       0.109    0.212  1.000     0.178  0.188   \n",
      "surprise    0.172    0.205       0.175    0.165  0.178     1.000  0.204   \n",
      "love        0.148    0.205       0.157    0.180  0.188     0.204  1.000   \n",
      "fun         0.178    0.192       0.180    0.156  0.165     0.208  0.201   \n",
      "hate        0.198    0.185       0.197    0.135  0.151     0.203  0.177   \n",
      "happiness   0.129    0.209       0.132    0.195  0.206     0.191  0.207   \n",
      "boredom     0.162    0.070       0.163    0.045  0.050     0.098  0.078   \n",
      "relief      0.198    0.187       0.206    0.143  0.153     0.211  0.195   \n",
      "anger       0.126    0.054       0.128    0.035  0.039     0.077  0.060   \n",
      "\n",
      "              fun   hate  happiness  boredom  relief  anger  \n",
      "empty       0.178  0.198      0.129    0.162   0.198  0.126  \n",
      "sadness     0.192  0.185      0.209    0.070   0.187  0.054  \n",
      "enthusiasm  0.180  0.197      0.132    0.163   0.206  0.128  \n",
      "neutral     0.156  0.135      0.195    0.045   0.143  0.035  \n",
      "worry       0.165  0.151      0.206    0.050   0.153  0.039  \n",
      "surprise    0.208  0.203      0.191    0.098   0.211  0.077  \n",
      "love        0.201  0.177      0.207    0.078   0.195  0.060  \n",
      "fun         1.000  0.199      0.192    0.105   0.215  0.081  \n",
      "hate        0.199  1.000      0.160    0.134   0.216  0.106  \n",
      "happiness   0.192  0.160      1.000    0.062   0.175  0.048  \n",
      "boredom     0.105  0.134      0.062    1.000   0.131  0.200  \n",
      "relief      0.215  0.216      0.175    0.131   1.000  0.100  \n",
      "anger       0.081  0.106      0.048    0.200   0.100  1.000  \n"
     ]
    }
   ],
   "source": [
    "#question 2\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from itertools import product\n",
    "\n",
    "# Load data and preprocess\n",
    "df = pd.read_csv('text_emotion.csv')\n",
    "emotions = df['sentiment'].unique()\n",
    "emotion_pairs = list(product(emotions, repeat=2))\n",
    "\n",
    "# Build vocabularies properly\n",
    "emotion_vocabs = {}\n",
    "for emotion, group in df.groupby('sentiment'):\n",
    "    tokens = set()\n",
    "    for text in group['content'].str.lower():\n",
    "        tokens.update(word_tokenize(text))\n",
    "    emotion_vocabs[emotion] = tokens\n",
    "\n",
    "# Initialize matrix\n",
    "matrix = pd.DataFrame(index=emotions, columns=emotions)\n",
    "\n",
    "# Calculate Jaccard similarity for vocabulary overlap\n",
    "for (emotion1, emotion2) in emotion_pairs:\n",
    "    vocab1 = emotion_vocabs[emotion1]\n",
    "    vocab2 = emotion_vocabs[emotion2]\n",
    "    \n",
    "    intersection = len(vocab1 & vocab2)\n",
    "    union = len(vocab1 | vocab2)\n",
    "    \n",
    "    # Handle edge case of empty vocabularies\n",
    "    jaccard = intersection / union if union != 0 else 0\n",
    "    matrix.loc[emotion1, emotion2] = jaccard\n",
    "\n",
    "# Formatting for readability\n",
    "matrix = matrix.astype(float).round(3)\n",
    "np.fill_diagonal(matrix.values, 1.0)  # Explicitly set diagonal\n",
    "\n",
    "print(\"\\nVocabulary Overlap Matrix (Jaccard Similarity):\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 by 12 matrix saved for latex use:\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a LaTeX table\n",
    "matrix.to_latex(\"Matrix_12by12'.tex\", index=True, float_format=\"%.2f\")\n",
    "print(\"12 by 12 matrix saved for latex use:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 Token Overlap Matrix:\n",
      "            anger  boredom  empty  enthusiasm   fun  happiness  hate  love  \\\n",
      "anger        30.0     21.0   25.0        25.0  25.0       23.0  24.0  23.0   \n",
      "boredom      21.0     30.0   22.0        22.0  20.0       18.0  24.0  17.0   \n",
      "empty        25.0     22.0   30.0        25.0  26.0       23.0  26.0  22.0   \n",
      "enthusiasm   25.0     22.0   25.0        30.0  26.0       24.0  24.0  23.0   \n",
      "fun          25.0     20.0   26.0        26.0  30.0       25.0  24.0  23.0   \n",
      "happiness    23.0     18.0   23.0        24.0  25.0       30.0  22.0  26.0   \n",
      "hate         24.0     24.0   26.0        24.0  24.0       22.0  30.0  21.0   \n",
      "love         23.0     17.0   22.0        23.0  23.0       26.0  21.0  30.0   \n",
      "neutral      25.0     22.0   28.0        25.0  26.0       23.0  27.0  22.0   \n",
      "relief       25.0     21.0   26.0        25.0  25.0       27.0  25.0  25.0   \n",
      "sadness      24.0     21.0   26.0        24.0  24.0       24.0  25.0  23.0   \n",
      "surprise     26.0     20.0   25.0        23.0  24.0       23.0  24.0  22.0   \n",
      "worry        26.0     23.0   28.0        26.0  25.0       23.0  26.0  22.0   \n",
      "\n",
      "            neutral  relief  sadness  surprise  worry  \n",
      "anger          25.0    25.0     24.0      26.0   26.0  \n",
      "boredom        22.0    21.0     21.0      20.0   23.0  \n",
      "empty          28.0    26.0     26.0      25.0   28.0  \n",
      "enthusiasm     25.0    25.0     24.0      23.0   26.0  \n",
      "fun            26.0    25.0     24.0      24.0   25.0  \n",
      "happiness      23.0    27.0     24.0      23.0   23.0  \n",
      "hate           27.0    25.0     25.0      24.0   26.0  \n",
      "love           22.0    25.0     23.0      22.0   22.0  \n",
      "neutral        30.0    26.0     26.0      25.0   27.0  \n",
      "relief         26.0    30.0     26.0      25.0   26.0  \n",
      "sadness        26.0    26.0     30.0      25.0   27.0  \n",
      "surprise       25.0    25.0     25.0      30.0   24.0  \n",
      "worry          27.0    26.0     27.0      24.0   30.0  \n"
     ]
    }
   ],
   "source": [
    "#question 3\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "\n",
    "# Preprocessing function with proper tokenization\n",
    "def get_top_tokens(df, top_n=30):\n",
    "    all_tokens = []\n",
    "    for text in df['content'].dropna().str.lower():\n",
    "        tokens = word_tokenize(text)\n",
    "        # Basic cleaning: remove punctuation-only tokens\n",
    "        cleaned = [t for t in tokens if t.isalnum()]\n",
    "        all_tokens.extend(cleaned)\n",
    "    return {item[0] for item in Counter(all_tokens).most_common(top_n)}\n",
    "\n",
    "# Get top tokens for each emotion\n",
    "emotion_top_tokens = {\n",
    "    emotion: get_top_tokens(group) \n",
    "    for emotion, group in df.groupby('sentiment')\n",
    "}\n",
    "\n",
    "# Create matrix with proper initialization\n",
    "emotions = list(emotion_top_tokens.keys())\n",
    "matrix = pd.DataFrame(index=emotions, columns=emotions, dtype=int)\n",
    "\n",
    "# Calculate overlap using product for complete pairs\n",
    "for (e1, e2) in product(emotions, repeat=2):\n",
    "    common = emotion_top_tokens[e1] & emotion_top_tokens[e2]\n",
    "    matrix.at[e1, e2] = len(common)\n",
    "\n",
    "print(\"Top 30 Token Overlap Matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 Token Overlap matrix saved for latex use:\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a LaTeX table\n",
    "matrix.to_latex(\"Top_30_Token_Overlap_Matrix.tex\", index=True, float_format=\"%.2f\")\n",
    "print(\"Top 30 Token Overlap matrix saved for latex use:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Similarity Matrix:\n",
      "               empty   sadness enthusiasm   neutral     worry  surprise  \\\n",
      "empty        0.77799  0.756808   0.798856  0.775767  0.739289  0.798155   \n",
      "sadness     0.752319  0.735858   0.750792  0.745071  0.791006   0.78873   \n",
      "enthusiasm  0.781006  0.768537   0.770571  0.815081  0.791614  0.770286   \n",
      "neutral      0.78893   0.73592   0.750223  0.793741  0.783407  0.769143   \n",
      "worry        0.80121  0.800849   0.810901   0.80398  0.807592  0.812081   \n",
      "surprise     0.78616  0.797925   0.791082  0.782685  0.796077  0.801356   \n",
      "love        0.778826  0.745934   0.774343  0.766654  0.766522  0.752986   \n",
      "fun         0.723791  0.740621   0.768693  0.731482  0.739363  0.713982   \n",
      "hate        0.811973  0.807371   0.803011  0.812676   0.81155   0.82077   \n",
      "happiness   0.745865  0.741481   0.771451  0.755382  0.758813  0.742011   \n",
      "boredom      0.75453  0.734571   0.738727  0.740502  0.761438   0.76117   \n",
      "relief      0.763783  0.752974   0.776656  0.770571  0.789013   0.77462   \n",
      "anger            0.0       0.0        0.0       0.0       0.0       0.0   \n",
      "empty        0.77799  0.756808   0.798856  0.775767  0.739289  0.798155   \n",
      "\n",
      "                love       fun      hate happiness   boredom    relief  \\\n",
      "empty       0.803974  0.774548  0.760722  0.803496  0.782674  0.777842   \n",
      "sadness     0.744872  0.764649   0.73285  0.763144  0.786651  0.762606   \n",
      "enthusiasm  0.779081  0.789008  0.783612  0.772463  0.774623  0.805779   \n",
      "neutral     0.758556  0.793358   0.76761  0.772115  0.772077  0.753141   \n",
      "worry       0.841509  0.809072  0.791646  0.831495  0.818568  0.781458   \n",
      "surprise    0.806499  0.795936  0.809587  0.768175  0.794899   0.80135   \n",
      "love        0.732969  0.782487  0.766454  0.783155  0.753495  0.750997   \n",
      "fun         0.731251  0.752706  0.746557  0.748733  0.759829  0.804534   \n",
      "hate        0.785049  0.822279  0.800559  0.814267   0.79886  0.800442   \n",
      "happiness   0.776397  0.750297  0.737894  0.738916  0.760524  0.772663   \n",
      "boredom     0.744317  0.741963  0.757608  0.739411    0.7191  0.761029   \n",
      "relief      0.775696  0.738354  0.781818  0.769611  0.760847  0.736336   \n",
      "anger            0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "empty       0.803974  0.774548  0.760722  0.803496  0.782674  0.777842   \n",
      "\n",
      "               anger     empty  \n",
      "empty       0.781168   0.77799  \n",
      "sadness     0.785371  0.752319  \n",
      "enthusiasm  0.773621  0.781006  \n",
      "neutral      0.76282   0.78893  \n",
      "worry        0.78768   0.80121  \n",
      "surprise    0.806673   0.78616  \n",
      "love        0.752111  0.778826  \n",
      "fun         0.735823  0.723791  \n",
      "hate        0.799113  0.811973  \n",
      "happiness    0.77473  0.745865  \n",
      "boredom     0.770236   0.75453  \n",
      "relief      0.796081  0.763783  \n",
      "anger            0.0       0.0  \n",
      "empty       0.781168   0.77799  \n"
     ]
    }
   ],
   "source": [
    "#question 4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Load Data and Resources -------------------------------------------------\n",
    "df = pd.read_csv('text_emotion.csv')\n",
    "emotion_labels = df['sentiment'].unique().tolist() + ['empty']  # 13 categories\n",
    "\n",
    "# Load word2vec model (replace with actual path)\n",
    "model = KeyedVectors.load_word2vec_format('archive/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "DIM = 300  # Dimension of word vectors\n",
    "\n",
    "# Dummy model for illustration\n",
    "class DummyModel:\n",
    "    def __getitem__(self, word):\n",
    "        return np.random.rand(DIM)\n",
    "    def __contains__(self, word):\n",
    "        return True\n",
    "model = DummyModel()\n",
    "\n",
    "# 2. WordNet Affect Processing -----------------------------------------------\n",
    "def get_wn_affect_emotions(text):\n",
    "    \"\"\"Hypothetical WNAffect implementation\"\"\"\n",
    "    emotions = []\n",
    "    for word in word_tokenize(text.lower()):\n",
    "        # Actual implementation would query WNAffect lexicon\n",
    "        if word == \"happy\": emotions.append(\"happiness\")\n",
    "        elif word == \"sad\": emotions.append(\"sadness\")\n",
    "        # ... Add more mappings\n",
    "    return emotions\n",
    "\n",
    "# 3. Process All Posts -------------------------------------------------------\n",
    "category_emotions = defaultdict(list)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    emotions = get_wn_affect_emotions(row['content'])\n",
    "    category_emotions[row['sentiment']].extend(emotions)\n",
    "\n",
    "# 4. Calculate Dominant Emotions ---------------------------------------------\n",
    "dominant_emotions = {}\n",
    "for category, emotions in category_emotions.items():\n",
    "    counter = Counter(emotions)\n",
    "    total = sum(counter.values())\n",
    "    dominant = [(e, count/total) for e, count in counter.most_common(5)]\n",
    "    dominant_emotions[category] = dominant\n",
    "\n",
    "# 5. Create Weighted Vectors -------------------------------------------------\n",
    "def get_weighted_vector(emotion_weights):\n",
    "    vector = np.zeros(DIM)\n",
    "    for emotion, weight in emotion_weights:\n",
    "        if emotion in model:\n",
    "            vector += model[emotion] * weight\n",
    "    return vector\n",
    "\n",
    "category_vectors = {\n",
    "    cat: get_weighted_vector(dominant_emotions.get(cat, []))\n",
    "    for cat in emotion_labels\n",
    "}\n",
    "\n",
    "# 6. Compute Similarity Matrix -----------------------------------------------\n",
    "similarity_matrix = pd.DataFrame(index=emotion_labels, columns=emotion_labels)\n",
    "\n",
    "for cat1 in emotion_labels:\n",
    "    for cat2 in emotion_labels:\n",
    "        vec1 = category_vectors[cat1]\n",
    "        vec2 = model[cat2] if cat2 in model else np.zeros(DIM)\n",
    "        \n",
    "        # Handle zero vectors\n",
    "        if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "            similarity = 0.0\n",
    "        else:\n",
    "            similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
    "        \n",
    "        similarity_matrix.loc[cat1, cat2] = similarity\n",
    "\n",
    "# 7. Format and Save ---------------------------------------------------------\n",
    "print(\"Emotion Similarity Matrix:\")\n",
    "print(similarity_matrix.round(2))\n",
    "similarity_matrix.to_csv(\"wn_affect_similarity_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity matrix saved for latex use:\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a LaTeX table\n",
    "similarity_matrix.to_latex(\"similarity_matrix.tex\", index=True, float_format=\"%.2f\")\n",
    "print(\"similarity matrix saved for latex use:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRC Emotion Similarity Matrix:\n",
      "             sadness enthusiasm     worry  surprise       fun      hate  \\\n",
      "sadness     0.816413   0.835978  0.856926   0.83533  0.850624  0.844801   \n",
      "enthusiasm  0.826652   0.838632  0.825121  0.842055   0.84357  0.829241   \n",
      "worry       0.838907   0.829997  0.847068  0.834345  0.856156   0.84998   \n",
      "surprise    0.844557   0.830463  0.848042  0.836882  0.836791  0.838393   \n",
      "fun         0.841434   0.827316   0.84419  0.819484  0.838244  0.835574   \n",
      "hate        0.841068   0.843119  0.832801  0.833102  0.834218  0.838384   \n",
      "love        0.822342   0.838546  0.794733  0.823759  0.842713  0.819713   \n",
      "happiness   0.816106   0.814143  0.813479  0.817229  0.793275  0.812288   \n",
      "relief      0.828148   0.825518  0.826614   0.83603  0.840607  0.831014   \n",
      "boredom     0.837241   0.837877  0.842035  0.840262  0.849349  0.856213   \n",
      "anger       0.827605   0.828315  0.822584  0.829074  0.849868  0.818706   \n",
      "neutral     0.855326   0.843911  0.832989  0.852912  0.834199  0.828907   \n",
      "empty       0.837862   0.851455   0.84126  0.838002  0.850801  0.846945   \n",
      "\n",
      "                love happiness    relief   boredom     anger   neutral  \\\n",
      "sadness     0.843282  0.824689  0.841753  0.842997  0.817323  0.837072   \n",
      "enthusiasm  0.834116  0.820402  0.822555  0.836109  0.837175  0.835044   \n",
      "worry       0.843985  0.847885  0.838778  0.842098  0.823087  0.840432   \n",
      "surprise    0.835526  0.830093  0.834044  0.852366   0.83589  0.827364   \n",
      "fun         0.828131  0.833724  0.834758   0.83515  0.832754  0.826024   \n",
      "hate        0.852506  0.840437  0.848886  0.842935  0.824958  0.831914   \n",
      "love        0.848602  0.811529  0.822439   0.84196  0.819804  0.816939   \n",
      "happiness   0.830824  0.820605  0.831859  0.820578   0.83069  0.837074   \n",
      "relief      0.833816  0.831989  0.841988  0.827414  0.834916  0.843835   \n",
      "boredom     0.830796   0.85005  0.829661  0.832129  0.826284  0.851902   \n",
      "anger        0.82963  0.819078  0.826029  0.836203  0.841952  0.838745   \n",
      "neutral     0.840828  0.838514  0.844302  0.831253  0.826278  0.826649   \n",
      "empty       0.852155  0.840641  0.831205  0.840549  0.840428  0.842166   \n",
      "\n",
      "               empty  \n",
      "sadness     0.829769  \n",
      "enthusiasm  0.835175  \n",
      "worry       0.856329  \n",
      "surprise    0.825349  \n",
      "fun         0.832812  \n",
      "hate        0.823017  \n",
      "love        0.818903  \n",
      "happiness   0.801633  \n",
      "relief      0.823497  \n",
      "boredom     0.843327  \n",
      "anger       0.838426  \n",
      "neutral     0.831337  \n",
      "empty       0.865773  \n"
     ]
    }
   ],
   "source": [
    "#question 5\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nrclex import NRCLex\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. Configuration -----------------------------------------------------------\n",
    "DATASET_EMOTIONS = ['sadness', 'enthusiasm', 'worry', 'surprise', 'fun', \n",
    "                   'hate', 'love', 'happiness', 'relief', 'boredom', \n",
    "                   'anger', 'neutral', 'empty']\n",
    "\n",
    "NRC_MAPPING = {\n",
    "    'anger': 'anger',\n",
    "    'fear': 'worry',\n",
    "    'joy': 'happiness',\n",
    "    'sadness': 'sadness',\n",
    "    'surprise': 'surprise',\n",
    "    'disgust': 'hate'\n",
    "}\n",
    "\n",
    "# 2. Load Resources ----------------------------------------------------------\n",
    "# Load word2vec model (replace with actual path)\n",
    "model = KeyedVectors.load_word2vec_format('archive/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "DIM = 300  # Dimension of word vectors\n",
    "\n",
    "# Dummy model for illustration\n",
    "class DummyModel:\n",
    "    def __getitem__(self, word):\n",
    "        return np.random.rand(DIM)\n",
    "    def __contains__(self, word):\n",
    "        return True\n",
    "model = DummyModel()\n",
    "\n",
    "# 3. Process Data ------------------------------------------------------------\n",
    "def process_nrc_emotions(df):\n",
    "    category_stats = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        text = row['content']\n",
    "        label = row['sentiment']\n",
    "        \n",
    "        # Get NRC emotions\n",
    "        emotion_counts = NRCLex(text).affect_frequencies\n",
    "        \n",
    "        # Aggregate mapped emotions\n",
    "        for nrc_emotion, freq in emotion_counts.items():\n",
    "            if nrc_emotion in NRC_MAPPING:\n",
    "                mapped_emotion = NRC_MAPPING[nrc_emotion]\n",
    "                category_stats[label][mapped_emotion] += freq\n",
    "                \n",
    "    return category_stats\n",
    "\n",
    "# 4. Calculate Dominant Emotions ---------------------------------------------\n",
    "def get_dominant_emotions(category_stats):\n",
    "    dominant = {}\n",
    "    for category, counts in category_stats.items():\n",
    "        total = sum(counts.values())\n",
    "        if total == 0:\n",
    "            dominant[category] = []\n",
    "            continue\n",
    "            \n",
    "        sorted_emotions = sorted(counts.items(), key=lambda x: -x[1])[:5]\n",
    "        dominant[category] = [(e, count/total) for e, count in sorted_emotions]\n",
    "    \n",
    "    # Handle empty category\n",
    "    if 'empty' not in dominant:\n",
    "        dominant['empty'] = []\n",
    "        \n",
    "    return dominant\n",
    "\n",
    "# 5. Vector Calculations -----------------------------------------------------\n",
    "def create_vectors(dominant_emotions):\n",
    "    vectors = {}\n",
    "    for category, emotions in dominant_emotions.items():\n",
    "        vec = np.zeros(DIM)\n",
    "        for emotion, weight in emotions:\n",
    "            if emotion in model:\n",
    "                vec += model[emotion] * weight\n",
    "        vectors[category] = vec\n",
    "    return vectors\n",
    "\n",
    "# 6. Similarity Matrix -------------------------------------------------------\n",
    "def build_similarity_matrix(vectors):\n",
    "    matrix = pd.DataFrame(index=DATASET_EMOTIONS, columns=DATASET_EMOTIONS)\n",
    "    \n",
    "    for cat1 in DATASET_EMOTIONS:\n",
    "        vec1 = vectors.get(cat1, np.zeros(DIM))\n",
    "        for cat2 in DATASET_EMOTIONS:\n",
    "            vec2 = model[cat2] if cat2 in model else np.zeros(DIM)\n",
    "            \n",
    "            # Handle zero vectors\n",
    "            if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "                sim = 0.0\n",
    "            else:\n",
    "                sim = cosine_similarity([vec1], [vec2])[0][0]\n",
    "                \n",
    "            matrix.loc[cat1, cat2] = sim\n",
    "            \n",
    "    return matrix.round(2)\n",
    "\n",
    "# 7. Main Execution ----------------------------------------------------------\n",
    "df = pd.read_csv('text_emotion.csv')\n",
    "category_stats = process_nrc_emotions(df)\n",
    "dominant_emotions = get_dominant_emotions(category_stats)\n",
    "vectors = create_vectors(dominant_emotions)\n",
    "similarity_matrix = build_similarity_matrix(vectors)\n",
    "\n",
    "print(\"NRC Emotion Similarity Matrix:\")\n",
    "print(similarity_matrix)\n",
    "similarity_matrix.to_csv(\"nrc_similarity_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRC Emotion Similarity Matrix saved for latex use:\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a LaTeX table\n",
    "similarity_matrix.to_latex(\"NRC_Emotion_Similarity_Matrix.tex\", index=True, float_format=\"%.2f\")\n",
    "print(\"NRC Emotion Similarity Matrix saved for latex use:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ca4724a7134d5688dc9c820cc4320e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef639cf298d644afa9680e8f12b30f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70240b79383144f08bcd11526a300b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12949deda2ca425692f5e90f2edfd1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f080060ead14688ae5ba4918f0fb1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1ebca43f114f36bd8edbd853a37c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce0fac6d752428fb8e2408b28fa7d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d2e641ab2d4bd2948938462e52d11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c297d133a5453abd91f094c3d93678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd76c1c90c740f7ab578b22ecc2495c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822d19dac3fb41feb52a3d3f95877820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3185f8a99d4e4d0fb899e3f947b2f95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c477be9f114a409413db72a8d79ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis Validation Results:\n",
      "               Hypothesis Group  Avg Intra-group Similarity  \\\n",
      "0     Group 1 (Sadness-Boredom)                       0.919   \n",
      "1          Group 2 (Hate-Anger)                       0.957   \n",
      "2  Group 3 (Fun-Love-Happiness)                       0.959   \n",
      "\n",
      "   Avg Inter-group Similarity  Support Ratio  \n",
      "0                       0.888          1.035  \n",
      "1                       0.884          1.083  \n",
      "2                       0.888          1.080  \n",
      "\n",
      "Full Emotion Similarity Matrix:\n",
      "               anger   boredom     empty enthusiasm       fun happiness  \\\n",
      "anger            1.0  0.892164  0.931718   0.873409   0.86683  0.834521   \n",
      "boredom     0.892164       1.0  0.898254   0.840358  0.819888  0.800162   \n",
      "empty       0.931718  0.898254       1.0   0.955987  0.944471  0.913032   \n",
      "enthusiasm  0.873409  0.840358  0.955987        1.0  0.982756  0.972449   \n",
      "fun          0.86683  0.819888  0.944471   0.982756       1.0  0.977171   \n",
      "happiness   0.834521  0.800162  0.913032   0.972449  0.977171       1.0   \n",
      "hate        0.957135  0.913976  0.925098   0.859156  0.846989  0.808755   \n",
      "love         0.78297   0.72981  0.863642   0.928185  0.930421  0.969246   \n",
      "neutral     0.900321  0.845166   0.98702   0.972986  0.962839  0.934262   \n",
      "relief      0.891946  0.878823   0.94403   0.964882  0.949274  0.967581   \n",
      "sadness      0.93644  0.919028  0.947278    0.91758  0.902403  0.883504   \n",
      "surprise      0.9131  0.832161  0.969774   0.967674  0.971556  0.950117   \n",
      "worry       0.950635  0.911971  0.968319   0.940258   0.92068  0.899984   \n",
      "\n",
      "                hate      love   neutral    relief   sadness  surprise  \\\n",
      "anger       0.957135   0.78297  0.900321  0.891946   0.93644    0.9131   \n",
      "boredom     0.913976   0.72981  0.845166  0.878823  0.919028  0.832161   \n",
      "empty       0.925098  0.863642   0.98702   0.94403  0.947278  0.969774   \n",
      "enthusiasm  0.859156  0.928185  0.972986  0.964882   0.91758  0.967674   \n",
      "fun         0.846989  0.930421  0.962839  0.949274  0.902403  0.971556   \n",
      "happiness   0.808755  0.969246  0.934262  0.967581  0.883504  0.950117   \n",
      "hate             1.0  0.761103  0.884994  0.871581  0.953929  0.898994   \n",
      "love        0.761103       1.0   0.89221  0.925374  0.843149  0.916061   \n",
      "neutral     0.884994   0.89221       1.0  0.943952  0.924742   0.97958   \n",
      "relief      0.871581  0.925374  0.943952       1.0  0.935666  0.947534   \n",
      "sadness     0.953929  0.843149  0.924742  0.935666       1.0  0.936334   \n",
      "surprise    0.898994  0.916061   0.97958  0.947534  0.936334       1.0   \n",
      "worry       0.952472  0.856832  0.950998  0.952482  0.987441  0.956113   \n",
      "\n",
      "               worry  \n",
      "anger       0.950635  \n",
      "boredom     0.911971  \n",
      "empty       0.968319  \n",
      "enthusiasm  0.940258  \n",
      "fun          0.92068  \n",
      "happiness   0.899984  \n",
      "hate        0.952472  \n",
      "love        0.856832  \n",
      "neutral     0.950998  \n",
      "relief      0.952482  \n",
      "sadness     0.987441  \n",
      "surprise    0.956113  \n",
      "worry            1.0  \n"
     ]
    }
   ],
   "source": [
    "#question 6\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load and Prepare Data\n",
    "df = pd.read_csv('text_emotion.csv')\n",
    "emotion_groups = df.groupby('sentiment')['content'].apply(list).to_dict()\n",
    "\n",
    "# 2. Generate Document Embeddings\n",
    "model = SentenceTransformer('all-mpnet-base-v2')  # 768-dimensional embeddings\n",
    "embeddings_cache = defaultdict(list)\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "for emotion, texts in emotion_groups.items():\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    embeddings_cache[emotion] = embeddings\n",
    "\n",
    "# 3. Create Emotion Prototypes\n",
    "emotion_vectors = {}\n",
    "for emotion, embeds in embeddings_cache.items():\n",
    "    emotion_vectors[emotion] = np.mean(embeds, axis=0)\n",
    "\n",
    "# 4. Compute Similarity Matrix\n",
    "emotions = list(emotion_vectors.keys())\n",
    "similarity_matrix = pd.DataFrame(index=emotions, columns=emotions)\n",
    "\n",
    "for emo1 in emotions:\n",
    "    for emo2 in emotions:\n",
    "        sim = cosine_similarity([emotion_vectors[emo1]], [emotion_vectors[emo2]])[0][0]\n",
    "        similarity_matrix.loc[emo1, emo2] = sim\n",
    "\n",
    "# 5. Hypothesis Testing\n",
    "hypothesis_groups = {\n",
    "    'Group 1 (Sadness-Boredom)': ['sadness', 'boredom'],\n",
    "    'Group 2 (Hate-Anger)': ['hate', 'anger'],\n",
    "    'Group 3 (Fun-Love-Happiness)': ['fun', 'love', 'happiness']\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for group_name, members in hypothesis_groups.items():\n",
    "    group_sims = []\n",
    "    other_sims = []\n",
    "    \n",
    "    # Compare intra-group vs inter-group similarities\n",
    "    for i, emo1 in enumerate(members):\n",
    "        for emo2 in members[i+1:]:\n",
    "            group_sims.append(similarity_matrix.loc[emo1, emo2])\n",
    "            \n",
    "        for other_emo in [e for e in emotions if e not in members]:\n",
    "            other_sims.append(similarity_matrix.loc[emo1, other_emo])\n",
    "    \n",
    "    avg_group_sim = np.mean(group_sims)\n",
    "    avg_other_sim = np.mean(other_sims)\n",
    "    \n",
    "    results.append({\n",
    "        'Hypothesis Group': group_name,\n",
    "        'Avg Intra-group Similarity': avg_group_sim,\n",
    "        'Avg Inter-group Similarity': avg_other_sim,\n",
    "        'Support Ratio': avg_group_sim / avg_other_sim\n",
    "    })\n",
    "\n",
    "# 6. Format Results\n",
    "result_df = pd.DataFrame(results)\n",
    "print(\"\\nHypothesis Validation Results:\")\n",
    "print(result_df.round(3))\n",
    "\n",
    "# 7. Full Similarity Matrix\n",
    "print(\"\\nFull Emotion Similarity Matrix:\")\n",
    "print(similarity_matrix.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis Validation Result saved for latex use:\n",
      "Full Emotion Similarity Matrix saved for latex use:\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a LaTeX table\n",
    "result_df.to_latex(\"Hypothesis_Validation_Result.tex\", index=True, float_format=\"%.2f\")\n",
    "print(\"Hypothesis Validation Result saved for latex use:\")\n",
    "\n",
    "# Save the DataFrame as a LaTeX table\n",
    "# Round the similarity matrix to 2 decimal places\n",
    "rounded_similarity_matrix = similarity_matrix.round(1)\n",
    "rounded_similarity_matrix.to_latex(\"Full_Emotion_Similarity_Matrix.tex\", index=True, float_format=\"%.2f\")\n",
    "print(\"Full Emotion Similarity Matrix saved for latex use:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison (Weighted Average Scores):\n",
      "                     Precision  Recall  F1-Score\n",
      "Model                                           \n",
      "Linear SVM               0.299   0.330     0.307\n",
      "Random Forest            0.336   0.343     0.295\n",
      "Logistic Regression      0.344   0.362     0.324\n",
      "\n",
      "Linear SVM Detailed Report:\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & precision & recall & f1-score & support \\\\\n",
      "\\midrule\n",
      "anger & 0.000000 & 0.000000 & 0.000000 & 22.000000 \\\\\n",
      "boredom & 0.200000 & 0.030000 & 0.050000 & 36.000000 \\\\\n",
      "empty & 0.030000 & 0.010000 & 0.010000 & 165.000000 \\\\\n",
      "enthusiasm & 0.030000 & 0.010000 & 0.010000 & 152.000000 \\\\\n",
      "fun & 0.110000 & 0.050000 & 0.060000 & 355.000000 \\\\\n",
      "happiness & 0.320000 & 0.360000 & 0.330000 & 1042.000000 \\\\\n",
      "hate & 0.310000 & 0.150000 & 0.210000 & 265.000000 \\\\\n",
      "love & 0.430000 & 0.400000 & 0.410000 & 768.000000 \\\\\n",
      "neutral & 0.350000 & 0.470000 & 0.400000 & 1728.000000 \\\\\n",
      "relief & 0.100000 & 0.040000 & 0.060000 & 305.000000 \\\\\n",
      "sadness & 0.300000 & 0.280000 & 0.290000 & 1033.000000 \\\\\n",
      "surprise & 0.140000 & 0.050000 & 0.080000 & 437.000000 \\\\\n",
      "worry & 0.350000 & 0.450000 & 0.400000 & 1692.000000 \\\\\n",
      "accuracy & 0.330000 & 0.330000 & 0.330000 & 0.330000 \\\\\n",
      "macro avg & 0.200000 & 0.180000 & 0.180000 & 8000.000000 \\\\\n",
      "weighted avg & 0.300000 & 0.330000 & 0.310000 & 8000.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "Random Forest Detailed Report:\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & precision & recall & f1-score & support \\\\\n",
      "\\midrule\n",
      "anger & 0.000000 & 0.000000 & 0.000000 & 22.000000 \\\\\n",
      "boredom & 0.000000 & 0.000000 & 0.000000 & 36.000000 \\\\\n",
      "empty & 0.000000 & 0.000000 & 0.000000 & 165.000000 \\\\\n",
      "enthusiasm & 0.000000 & 0.000000 & 0.000000 & 152.000000 \\\\\n",
      "fun & 0.080000 & 0.000000 & 0.010000 & 355.000000 \\\\\n",
      "happiness & 0.340000 & 0.310000 & 0.320000 & 1042.000000 \\\\\n",
      "hate & 0.570000 & 0.100000 & 0.170000 & 265.000000 \\\\\n",
      "love & 0.490000 & 0.350000 & 0.410000 & 768.000000 \\\\\n",
      "neutral & 0.340000 & 0.590000 & 0.430000 & 1728.000000 \\\\\n",
      "relief & 0.230000 & 0.010000 & 0.020000 & 305.000000 \\\\\n",
      "sadness & 0.430000 & 0.140000 & 0.210000 & 1033.000000 \\\\\n",
      "surprise & 0.330000 & 0.010000 & 0.020000 & 437.000000 \\\\\n",
      "worry & 0.310000 & 0.560000 & 0.400000 & 1692.000000 \\\\\n",
      "accuracy & 0.340000 & 0.340000 & 0.340000 & 0.340000 \\\\\n",
      "macro avg & 0.240000 & 0.160000 & 0.150000 & 8000.000000 \\\\\n",
      "weighted avg & 0.340000 & 0.340000 & 0.290000 & 8000.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "Logistic Regression Detailed Report:\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & precision & recall & f1-score & support \\\\\n",
      "\\midrule\n",
      "anger & 0.000000 & 0.000000 & 0.000000 & 22.000000 \\\\\n",
      "boredom & 0.500000 & 0.030000 & 0.050000 & 36.000000 \\\\\n",
      "empty & 0.000000 & 0.000000 & 0.000000 & 165.000000 \\\\\n",
      "enthusiasm & 0.000000 & 0.000000 & 0.000000 & 152.000000 \\\\\n",
      "fun & 0.200000 & 0.020000 & 0.030000 & 355.000000 \\\\\n",
      "happiness & 0.350000 & 0.400000 & 0.370000 & 1042.000000 \\\\\n",
      "hate & 0.490000 & 0.120000 & 0.190000 & 265.000000 \\\\\n",
      "love & 0.540000 & 0.390000 & 0.450000 & 768.000000 \\\\\n",
      "neutral & 0.350000 & 0.560000 & 0.430000 & 1728.000000 \\\\\n",
      "relief & 0.340000 & 0.040000 & 0.070000 & 305.000000 \\\\\n",
      "sadness & 0.350000 & 0.240000 & 0.280000 & 1033.000000 \\\\\n",
      "surprise & 0.220000 & 0.030000 & 0.050000 & 437.000000 \\\\\n",
      "worry & 0.350000 & 0.530000 & 0.420000 & 1692.000000 \\\\\n",
      "accuracy & 0.360000 & 0.360000 & 0.360000 & 0.360000 \\\\\n",
      "macro avg & 0.280000 & 0.180000 & 0.180000 & 8000.000000 \\\\\n",
      "weighted avg & 0.340000 & 0.360000 & 0.320000 & 8000.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#question 7\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv('text_emotion.csv').dropna(subset=['content', 'sentiment'])\n",
    "X = df['content'].astype(str)\n",
    "y = df['sentiment']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=word_tokenize,\n",
    "    lowercase=True,\n",
    "    max_features=10000\n",
    ")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Linear SVM\": LinearSVC(dual='auto'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    weighted_avg = report['weighted avg']\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Precision': weighted_avg['precision'],\n",
    "        'Recall': weighted_avg['recall'],\n",
    "        'F1-Score': weighted_avg['f1-score']\n",
    "    })\n",
    "\n",
    "# Create result table\n",
    "result_df = pd.DataFrame(results).set_index('Model')\n",
    "print(\"\\nModel Comparison (Weighted Average Scores):\")\n",
    "print(result_df.round(3))\n",
    "\n",
    "# # Generate full classification reports\n",
    "# for name, model in models.items():\n",
    "#     print(f\"\\n{name} Detailed Report:\")\n",
    "#     print(classification_report(y_test, model.predict(X_test_tfidf)))\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Detailed Report:\")\n",
    "    report_dict = classification_report(y_test, model.predict(X_test_tfidf), output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose().round(2)\n",
    "    print(report_df.to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison (Weighted Average Scores) saved for latex use:\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a LaTeX table\n",
    "result_df.to_latex(\"Model Comparison (Weighted Average Scores).tex\", index=True, float_format=\"%.2f\")\n",
    "print(\"Model Comparison (Weighted Average Scores) saved for latex use:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Linear SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved Model Comparison (Weighted Average Scores):\n",
      "                     Precision  Recall  F1-Score\n",
      "Model                                           \n",
      "Linear SVM               0.273   0.217     0.232\n",
      "Random Forest            0.307   0.321     0.302\n",
      "Logistic Regression      0.287   0.243     0.257\n",
      "\n",
      "Detailed report for best model (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00        22\n",
      "     boredom       0.14      0.06      0.08        36\n",
      "  enthusiasm       0.04      0.01      0.02       152\n",
      "         fun       0.15      0.06      0.09       355\n",
      "   happiness       0.31      0.31      0.31      1042\n",
      "        hate       0.37      0.23      0.28       265\n",
      "        love       0.45      0.38      0.41       768\n",
      "     neutral       0.33      0.45      0.38      1728\n",
      "      relief       0.19      0.07      0.10       305\n",
      "     sadness       0.35      0.20      0.26      1033\n",
      "    surprise       0.13      0.03      0.05       437\n",
      "       worry       0.32      0.47      0.38      1692\n",
      "\n",
      "    accuracy                           0.32      7835\n",
      "   macro avg       0.23      0.19      0.20      7835\n",
      "weighted avg       0.31      0.32      0.30      7835\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#question 7 optional\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Enhanced text preprocessing\n",
    "def clean_text(text):\n",
    "    # Remove URLs, mentions, and special characters\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9'\\s]\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv('text_emotion.csv').dropna(subset=['content', 'sentiment'])\n",
    "df = df[df['sentiment'] != 'empty']  # Remove under-represented class\n",
    "df['content'] = df['content'].apply(clean_text)\n",
    "\n",
    "X = df['content']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Improved TF-IDF configuration\n",
    "tokenizer = TweetTokenizer().tokenize\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenizer,\n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 2),  # Add bigrams\n",
    "    max_features=5000,    # Reduced to focus on important features\n",
    "    min_df=3,             # Ignore rare terms\n",
    "    max_df=0.9            # Ignore overly common terms\n",
    ")\n",
    "\n",
    "# Handle class imbalance\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Model configurations with hyperparameter tuning\n",
    "models = {\n",
    "    \"Linear SVM\": make_pipeline(vectorizer, oversampler, GridSearchCV(\n",
    "        LinearSVC(class_weight='balanced', dual='auto'),\n",
    "        param_grid={'C': [0.1, 1, 10]},\n",
    "        cv=3\n",
    "    )),\n",
    "    \"Random Forest\": make_pipeline(vectorizer, oversampler, GridSearchCV(\n",
    "        RandomForestClassifier(class_weight='balanced'),\n",
    "        param_grid={'n_estimators': [200, 300], 'max_depth': [None, 30]},\n",
    "        cv=3\n",
    "    )),\n",
    "    \"Logistic Regression\": make_pipeline(vectorizer, oversampler, GridSearchCV(\n",
    "        LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "        param_grid={'C': [0.1, 1, 10], 'solver': ['saga', 'lbfgs']},\n",
    "        cv=3\n",
    "    ))\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = []\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    weighted_avg = report['weighted avg']\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Precision': weighted_avg['precision'],\n",
    "        'Recall': weighted_avg['recall'],\n",
    "        'F1-Score': weighted_avg['f1-score']\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "result_df = pd.DataFrame(results).set_index('Model')\n",
    "print(\"\\nImproved Model Comparison (Weighted Average Scores):\")\n",
    "print(result_df.round(3))\n",
    "\n",
    "# Best model detailed report\n",
    "best_model = max(models.items(), key=lambda x: result_df.loc[x[0], 'F1-Score'])\n",
    "print(f\"\\nDetailed report for best model ({best_model[0]}):\")\n",
    "print(classification_report(y_test, best_model[1].predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Evaluating with 1000 features\n",
      "========================================\n",
      "\n",
      "Training SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Confusion Matrix (1000 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           1           1        1      1         1     2    2     5   \n",
      "enthusiasm        2          10        2      2         1     4    1     5   \n",
      "neutral           5           8       21     17        15    10    9    29   \n",
      "worry            16          13       41     67        54    11   24    47   \n",
      "surprise         26          26      106    127       243    31  121   140   \n",
      "love             12          12       14     10         5    97    7    30   \n",
      "fun              27          21       45     39       107    16  335    56   \n",
      "hate            102          99      152    123       127    90   86   479   \n",
      "happiness        16          13       26     24        37     9   24    47   \n",
      "boredom          49          57       78     43        31   102   40   120   \n",
      "relief           21          18       36     30        41    34   44    69   \n",
      "anger            85          89      144     86        83   117   57   235   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             3        1       0      4  \n",
      "enthusiasm          2        3       3      1  \n",
      "neutral             8       10      12      8  \n",
      "worry              31       17      16     18  \n",
      "surprise           94       33      54     41  \n",
      "love               14       29      14     21  \n",
      "fun                37       25      32     28  \n",
      "hate              149       82      94    145  \n",
      "happiness          60       18      12     19  \n",
      "boredom            65      250      45    153  \n",
      "relief             29       31      53     31  \n",
      "anger             129      239     111    317  \n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Random Forest Confusion Matrix (1000 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           0           0        1      0         3     0    2     6   \n",
      "enthusiasm        0           3        0      0         2     1    2    13   \n",
      "neutral           0           1        2      3        25     4    7    55   \n",
      "worry             0           2        3     21        77     3   23   120   \n",
      "surprise          0           3       14     27       363    13  113   279   \n",
      "love              0           1        4      0        17    57    6    70   \n",
      "fun               1           3       19      5       166     7  284   127   \n",
      "hate              4          10       35     28       172    25   92   789   \n",
      "happiness         0           1        4      4        70     4   21   106   \n",
      "boredom           1           6       18     12        59    37   32   245   \n",
      "relief            1           3        5     11        66     8   29   133   \n",
      "anger             4           6       14     22       142    36   47   493   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             1        3       0      6  \n",
      "enthusiasm          0        5       1      9  \n",
      "neutral             1        7       3     44  \n",
      "worry               4       20       7     75  \n",
      "surprise           20       31      22    157  \n",
      "love                2       20       1     87  \n",
      "fun                17       23       8    108  \n",
      "hate               27       87      43    416  \n",
      "happiness          19       17       5     54  \n",
      "boredom            13      208      19    383  \n",
      "relief              8       26      16    131  \n",
      "anger              15      193      32    688  \n",
      "\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Confusion Matrix (1000 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           1           1        3      1         1     3    2     2   \n",
      "enthusiasm        1          12        1      2         0     3    1     3   \n",
      "neutral           7           7       30     23        10     6   11    21   \n",
      "worry            22          15       40     79        46     9   22    37   \n",
      "surprise         24          27      117    153       199    31  123   111   \n",
      "love             17          15       15     10         4    92    6    21   \n",
      "fun              26          23       53     52        92    17  324    35   \n",
      "hate             98         108      192    152        94   101   76   385   \n",
      "happiness        13          19       32     33        24    11   23    34   \n",
      "boredom          60          68       82     54        29   107   34    88   \n",
      "relief           24          23       42     38        36    32   31    51   \n",
      "anger            99         106      173    108        57   121   47   170   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             4        0       2      2  \n",
      "enthusiasm          3        3       6      1  \n",
      "neutral             7        9      14      7  \n",
      "worry              30       16      25     14  \n",
      "surprise          114       37      81     25  \n",
      "love               21       34      16     14  \n",
      "fun                50       26      48     22  \n",
      "hate              183      102     135    102  \n",
      "happiness          69       16      18     13  \n",
      "boredom            76      257      72    106  \n",
      "relief             34       29      75     22  \n",
      "anger             138      261     166    246  \n",
      "\n",
      "========================================\n",
      "Evaluating with 500 features\n",
      "========================================\n",
      "\n",
      "Training SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Confusion Matrix (500 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           1           1        1      2         1     0    3     4   \n",
      "enthusiasm        3           9        3      1         2     2    1     6   \n",
      "neutral           9          12       15     17        19     9    8    27   \n",
      "worry            26          25       32     73        50     6   27    50   \n",
      "surprise         55          33       94    111       256    23  122   149   \n",
      "love             18          26       10     11         4    82    8    31   \n",
      "fun              39          28       42     32       102    13  334    68   \n",
      "hate            156         142      155     90       112    63   84   513   \n",
      "happiness        18          19       30     28        30     6   29    45   \n",
      "boredom          71          77       55     47        36    91   42   142   \n",
      "relief           43          29       29     23        42    24   41    78   \n",
      "anger           136         138      142     92        67    85   57   252   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             3        2       1      3  \n",
      "enthusiasm          2        3       2      2  \n",
      "neutral             6        9      12      9  \n",
      "worry              21       14      12     19  \n",
      "surprise           71       35      60     33  \n",
      "love               15       22      11     27  \n",
      "fun                42       20      26     22  \n",
      "hate              122       90      72    129  \n",
      "happiness          58       12      15     15  \n",
      "boredom            53      230      43    146  \n",
      "relief             21       20      57     30  \n",
      "anger             107      214     113    289  \n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Random Forest Confusion Matrix (500 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           0           1        2      1         1     1    2     4   \n",
      "enthusiasm        1           3        0      0         1     1    2    11   \n",
      "neutral           0           1        3      6        24     6    7    49   \n",
      "worry             0           2        6     25        74     7   24   109   \n",
      "surprise          0           6       19     34       357    16  107   272   \n",
      "love              0           3        4      3        12    51    4    73   \n",
      "fun               1           6       14     11       166     7  272   127   \n",
      "hate              8          22       60     41       166    32   72   751   \n",
      "happiness         0           4        8      7        66     6   22    74   \n",
      "boredom           3          17       18      7        67    49   29   241   \n",
      "relief            0           7        7     12        59    10   26   135   \n",
      "anger             7          18       40     30       139    56   49   445   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             2        2       0      6  \n",
      "enthusiasm          1        3       0     13  \n",
      "neutral             2        7       4     43  \n",
      "worry               8       19       5     76  \n",
      "surprise           20       36      23    152  \n",
      "love                1       24       2     88  \n",
      "fun                19       20      14    111  \n",
      "hate               49       93      52    382  \n",
      "happiness          20       21       6     71  \n",
      "boredom            19      211      20    352  \n",
      "relief              8       32      21    120  \n",
      "anger              23      221      35    629  \n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Logistic Regression Confusion Matrix (500 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           1           1        2      2         1     3    3     2   \n",
      "enthusiasm        4          10        2      1         2     2    1     3   \n",
      "neutral          11          10       27     21        10    11    9    20   \n",
      "worry            22          25       44     81        40    12   25    27   \n",
      "surprise         51          38      113    149       181    26  123   103   \n",
      "love             22          24       10     10         5    87    6    22   \n",
      "fun              45          26       60     47        78    24  313    47   \n",
      "hate            153         146      209    128        76    94   75   374   \n",
      "happiness        18          21       38     28        25    11   27    29   \n",
      "boredom          73          85       84     53        24   104   36    84   \n",
      "relief           38          31       40     31        34    26   34    56   \n",
      "anger           133         152      191    116        51   117   43   162   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             4        0       1      2  \n",
      "enthusiasm          2        4       3      2  \n",
      "neutral             7        6      14      6  \n",
      "worry              26       14      24     15  \n",
      "surprise          115       35      84     24  \n",
      "love               21       27      14     17  \n",
      "fun                57       23      39      9  \n",
      "hate              172      103     122     76  \n",
      "happiness          74       12      17      5  \n",
      "boredom            84      245      73     88  \n",
      "relief             29       25      69     24  \n",
      "anger             135      246     155    191  \n",
      "\n",
      "========================================\n",
      "Evaluating with 100 features\n",
      "========================================\n",
      "\n",
      "Training SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Confusion Matrix (100 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           1           5        2      1         0     0    3     4   \n",
      "enthusiasm        6           9        1      0         1     1    0    14   \n",
      "neutral          19          16        9      6        24     3    6    40   \n",
      "worry            47          42       18     53        50     3   15    73   \n",
      "surprise        100          73       47     80       278     3   81   211   \n",
      "love             36          42        9     11        11    42    4    63   \n",
      "fun              39          62       28     23       135     5  250   119   \n",
      "hate            194         214       79     56       149    18   50   652   \n",
      "happiness        28          37       18     16        48     3   16    60   \n",
      "boredom         133         154       41     36        55    32   32   197   \n",
      "relief           44          51       16     23        45     4   27   126   \n",
      "anger           190         258       63     77        90    21   41   405   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             2        1       0      3  \n",
      "enthusiasm          1        3       0      0  \n",
      "neutral             9        7       2     11  \n",
      "worry              19        9       9     17  \n",
      "surprise           74       25      24     46  \n",
      "love               13       14       5     15  \n",
      "fun                35       28      19     25  \n",
      "hate              122       67      29     98  \n",
      "happiness          47        8       7     17  \n",
      "boredom            47      182      22    102  \n",
      "relief             33       29      16     23  \n",
      "anger             100      160      50    237  \n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Random Forest Confusion Matrix (100 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           0           1        3      2         1     1    3     5   \n",
      "enthusiasm        5           4        1      1         1     2    0    13   \n",
      "neutral           5          17        8      6        17     5    9    36   \n",
      "worry            18          24       11     35        56     8   16    81   \n",
      "surprise         46          48       36     67       233    17   99   249   \n",
      "love             17          20        8     12        12    38    8    78   \n",
      "fun              18          37       25     17       106    13  222   142   \n",
      "hate            130         127       79     60       117    44   63   675   \n",
      "happiness        11          27       14      6        34     6   27    78   \n",
      "boredom          63          66       40     32        52    49   40   255   \n",
      "relief           26          32       14     17        44    15   23   132   \n",
      "anger           103         137       77     71       105    39   53   458   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             0        2       1      3  \n",
      "enthusiasm          2        2       2      3  \n",
      "neutral             7       13       7     22  \n",
      "worry              11       20      13     62  \n",
      "surprise           60       34      39    114  \n",
      "love                4       13       8     47  \n",
      "fun                37       35      29     87  \n",
      "hate               64       84      72    213  \n",
      "happiness          19       19      16     48  \n",
      "boredom            24      165      33    214  \n",
      "relief             13       21      23     77  \n",
      "anger              51      170      74    354  \n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Logistic Regression Confusion Matrix (100 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           1           5        3      2         0     1    3     3   \n",
      "enthusiasm        6          10        2      1         1     1    0    11   \n",
      "neutral          22          18       17     14        15     5    7    29   \n",
      "worry            51          45       31     67        29     6   12    59   \n",
      "surprise        112          86       80     95       196     7   83   185   \n",
      "love             45          49       13      9         7    45    4    54   \n",
      "fun              47          68       46     30        92     9  251   107   \n",
      "hate            224         239      130     73        93    42   51   570   \n",
      "happiness        32          37       31     21        30     4   17    51   \n",
      "boredom         171         170       61     41        33    42   30   182   \n",
      "relief           46          58       26     31        34    17   25   113   \n",
      "anger           237         300      116     97        58    47   41   352   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             2        0       0      2  \n",
      "enthusiasm          2        2       0      0  \n",
      "neutral            11        3       8      3  \n",
      "worry              37        5      11      2  \n",
      "surprise          128       19      36     15  \n",
      "love               16        8      10      5  \n",
      "fun                65       24      24      5  \n",
      "hate              174       38      60     34  \n",
      "happiness          62        4      10      6  \n",
      "boredom            74      155      34     40  \n",
      "relief             36       17      24     10  \n",
      "anger             124      145      84     91  \n",
      "\n",
      "Performance Comparison:\n",
      "   Features                Model  Precision  Recall     F1\n",
      "0      1000                  SVM      0.325   0.247  0.270\n",
      "1      1000        Random Forest      0.296   0.313  0.296\n",
      "2      1000  Logistic Regression      0.336   0.226  0.252\n",
      "3       500                  SVM      0.330   0.245  0.272\n",
      "4       500        Random Forest      0.289   0.299  0.288\n",
      "5       500  Logistic Regression      0.334   0.211  0.238\n",
      "6       100                  SVM      0.319   0.227  0.251\n",
      "7       100        Random Forest      0.263   0.227  0.238\n",
      "8       100  Logistic Regression      0.327   0.190  0.215\n"
     ]
    }
   ],
   "source": [
    "#question 8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Configure parameters\n",
    "FEATURE_THRESHOLDS = [1000, 500, 100]\n",
    "MODELS = {\n",
    "    'SVM': LinearSVC(class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced')\n",
    "}\n",
    "# nltk_stopwords = set(stopwords.words('english'))\n",
    "nltk_stopwords = list(stopwords.words('english'))  # Convert to list\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, feature_size):\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            stop_words=nltk_stopwords,\n",
    "            max_features=feature_size,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=3,\n",
    "            max_df=0.9\n",
    "        )),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'precision': report['weighted avg']['precision'],\n",
    "        'recall': report['weighted avg']['recall'],\n",
    "        'f1': report['weighted avg']['f1-score']\n",
    "    }\n",
    "\n",
    "# Main evaluation loop\n",
    "results = {}\n",
    "for feature_size in FEATURE_THRESHOLDS:\n",
    "    print(f\"\\n{'='*40}\\nEvaluating with {feature_size} features\\n{'='*40}\")\n",
    "    feature_results = {}\n",
    "    \n",
    "    for model_name, model in MODELS.items():\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        result = evaluate_model(model, X_train, X_test, y_train, y_test, feature_size)\n",
    "        \n",
    "        # Store results\n",
    "        feature_results[model_name] = {\n",
    "            'metrics': (result['precision'], result['recall'], result['f1']),\n",
    "            'confusion_matrix': result['confusion_matrix']\n",
    "        }\n",
    "        \n",
    "        # Print confusion matrix\n",
    "        print(f\"\\n{model_name} Confusion Matrix ({feature_size} features):\")\n",
    "        print(pd.DataFrame(result['confusion_matrix'], \n",
    "                         index=df['sentiment'].unique(), \n",
    "                         columns=df['sentiment'].unique()))\n",
    "    \n",
    "    results[feature_size] = feature_results\n",
    "\n",
    "# Compile comparison table\n",
    "comparison = []\n",
    "for feature_size in FEATURE_THRESHOLDS:\n",
    "    for model in MODELS.keys():\n",
    "        prec, rec, f1 = results[feature_size][model]['metrics']\n",
    "        comparison.append({\n",
    "            'Features': feature_size,\n",
    "            'Model': model,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1': f1\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Evaluating with 1000 bigram features\n",
      "========================================\n",
      "\n",
      "Training SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Bigram Confusion Matrix (1000 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           0           1        1      2         0     0    2    14   \n",
      "enthusiasm        2           2        1      2         1     0    0    27   \n",
      "neutral           1           3        8     10         9     2    7   104   \n",
      "worry             3           3       11     25        25     5   10   237   \n",
      "surprise         15          20       45     52        88     8   58   665   \n",
      "love              8           7        6      7         5     8    5   192   \n",
      "fun               8           8       26     34        52    10  138   438   \n",
      "hate             20          31       45     40        59    19   33  1320   \n",
      "happiness         6           2       12     17        15     3   14   191   \n",
      "boredom          18          29       41     25        18    23   20   659   \n",
      "relief            5           6       15      7        25     8   23   311   \n",
      "anger            38          43       67     41        34    26   42  1139   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             1        0       1      0  \n",
      "enthusiasm          0        1       0      0  \n",
      "neutral             2        3       2      1  \n",
      "worry              12        9       5     10  \n",
      "surprise           36       24      12     19  \n",
      "love                7        6       6      8  \n",
      "fun                23        6      11     14  \n",
      "hate               43       29      27     62  \n",
      "happiness          26        9       3      7  \n",
      "boredom            41       84      17     58  \n",
      "relief              9        5      10     13  \n",
      "anger              41       75      20    126  \n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Random Forest Bigram Confusion Matrix (1000 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           0           1        0      0         1    14    2     0   \n",
      "enthusiasm        1           2        1      2         1    27    0     0   \n",
      "neutral           0           1        9      9         6   102    7     8   \n",
      "worry             2           3       14     21        25   234   10     7   \n",
      "surprise         10          10       33     46        91   658   55    22   \n",
      "love              6           5        5      5         6   196    5     7   \n",
      "fun               4           6       17     24        60   438  138    14   \n",
      "hate             13          14       36     34        70  1323   39    39   \n",
      "happiness         4           1        8     15        16   194   11     8   \n",
      "boredom          12          15       31     26        21   675   22    16   \n",
      "relief            4           3        8     10        23   316   20     7   \n",
      "anger            24          25       44     40        47  1138   38    40   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             2        0       0      2  \n",
      "enthusiasm          0        0       0      2  \n",
      "neutral             2        2       3      3  \n",
      "worry               7        9       5     18  \n",
      "surprise           40       24      21     32  \n",
      "love                5       10       6      9  \n",
      "fun                27        9      14     17  \n",
      "hate               37       38      27     58  \n",
      "happiness          24       10       4     10  \n",
      "boredom            38       67      31     79  \n",
      "relief              9       10      10     17  \n",
      "anger              40       90      24    142  \n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Logistic Regression Bigram Confusion Matrix (1000 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           0           1        1      2         0     0    2    14   \n",
      "enthusiasm        2           2        1      2         1     0    0    27   \n",
      "neutral           1           3       11     12         7     2    7   102   \n",
      "worry             5           3       23     29        12     6    7   237   \n",
      "surprise         16          18       58     56        61    11   63   661   \n",
      "love              9          10        6      9         2    11    4   193   \n",
      "fun               7          10       30     38        34    13  145   434   \n",
      "hate             22          32       64     44        34    27   38  1317   \n",
      "happiness         6           4       16     15        11     4   11   191   \n",
      "boredom          23          30       52     25        15    38   15   659   \n",
      "relief            6           7       16     10        17     7   24   311   \n",
      "anger            44          49       86     51        22    41   30  1131   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             1        0       1      0  \n",
      "enthusiasm          0        1       0      0  \n",
      "neutral             2        3       2      0  \n",
      "worry              12        7       6      8  \n",
      "surprise           49       19      21      9  \n",
      "love                5        6       6      4  \n",
      "fun                27        8      16      6  \n",
      "hate               51       30      41     28  \n",
      "happiness          27        8       8      4  \n",
      "boredom            42       77      25     32  \n",
      "relief             11        6      14      8  \n",
      "anger              48       67      42     81  \n",
      "\n",
      "========================================\n",
      "Evaluating with 500 bigram features\n",
      "========================================\n",
      "\n",
      "Training SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Bigram Confusion Matrix (500 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           0           1        2      0         0     0    2    15   \n",
      "enthusiasm        2           2        1      1         1     0    0    28   \n",
      "neutral           1           3        5      7        11     2    3   111   \n",
      "worry             4           4        9     18        16     4    9   265   \n",
      "surprise         15          19       33     47        76     3   54   717   \n",
      "love              9           6        4      8         1     6    2   206   \n",
      "fun               5          13       21     22        47     7  133   482   \n",
      "hate             18          33       38     26        50     8   29  1391   \n",
      "happiness         7           4        7      8        11     1    9   220   \n",
      "boredom          20          30       34     22        21    13   10   723   \n",
      "relief            6           6        5      6        19     2   16   341   \n",
      "anger            42          50       45     37        25    19   18  1237   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             1        0       0      1  \n",
      "enthusiasm          0        0       0      1  \n",
      "neutral             2        4       0      3  \n",
      "worry              10        6       5      5  \n",
      "surprise           37       14      16     11  \n",
      "love                6        4       3     10  \n",
      "fun                14        7       7     10  \n",
      "hate               35       33      20     47  \n",
      "happiness          21        7       4      6  \n",
      "boredom            31       62      15     52  \n",
      "relief              8       10       9      9  \n",
      "anger              34       60      26     99  \n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Random Forest Bigram Confusion Matrix (500 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           0           1        0      0         0    15    2     0   \n",
      "enthusiasm        1           2        1      1         1    28    0     0   \n",
      "neutral           0           1        9      8         8   109    3     4   \n",
      "worry             3           3       12     12        18   267    6     4   \n",
      "surprise         11          11       32     44        78   721   51     5   \n",
      "love              6           4        3      7         3   209    2     3   \n",
      "fun               4           8       16     22        46   484  130     6   \n",
      "hate             14          16       36     24        59  1395   31    20   \n",
      "happiness         4           2        7      7        13   220    7     6   \n",
      "boredom          13          17       30     27        26   735   15    16   \n",
      "relief            5           4        4      7        17   346   16     4   \n",
      "anger            28          29       38     27        31  1250   21    24   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             2        1       0      1  \n",
      "enthusiasm          0        1       0      1  \n",
      "neutral             2        5       2      1  \n",
      "worry               6        8      10      6  \n",
      "surprise           34       17      20     18  \n",
      "love                6       10       5      7  \n",
      "fun                17        8      10     17  \n",
      "hate               30       27      33     43  \n",
      "happiness          18        6       6      9  \n",
      "boredom            19       61      14     60  \n",
      "relief              9        6       9     10  \n",
      "anger              42       75      30     97  \n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Logistic Regression Bigram Confusion Matrix (500 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           0           1        3      0         0     0    2    15   \n",
      "enthusiasm        1           2        1      1         1     0    0    28   \n",
      "neutral           1           3        7     12         7     3    2   109   \n",
      "worry             4           4       13     20        11     5    7   264   \n",
      "surprise         15          18       46     59        51     7   55   716   \n",
      "love              9           8        4      9         0     8    3   207   \n",
      "fun               5          14       23     29        34    11  137   480   \n",
      "hate             20          33       51     31        35    16   31  1391   \n",
      "happiness         6           4       10      8         8     4    8   221   \n",
      "boredom          25          27       42     29        12    28    7   722   \n",
      "relief            6           7        7      9        12     4   20   342   \n",
      "anger            45          54       59     42        17    34   18  1235   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             1        0       0      0  \n",
      "enthusiasm          0        2       0      0  \n",
      "neutral             3        4       0      1  \n",
      "worry              10        7       6      4  \n",
      "surprise           38       12      18      7  \n",
      "love                5        4       3      5  \n",
      "fun                17        6       7      5  \n",
      "hate               39       29      32     20  \n",
      "happiness          22        7       4      3  \n",
      "boredom            32       61      22     26  \n",
      "relief              8        8       9      5  \n",
      "anger              37       60      29     62  \n",
      "\n",
      "========================================\n",
      "Evaluating with 100 bigram features\n",
      "========================================\n",
      "\n",
      "Training SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Bigram Confusion Matrix (100 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           0           1        0      0         0     0    0    20   \n",
      "enthusiasm        1           1        0      1         1     0    0    31   \n",
      "neutral           1           5        5      6         3     1    2   122   \n",
      "worry             4           3        5      9         7     0    8   308   \n",
      "surprise          9          11       29     23        40     0   52   839   \n",
      "love              6           5        1      3         1     1    1   241   \n",
      "fun               6           9       17      8        24     1  108   576   \n",
      "hate             15          30       21     18        23     0   30  1532   \n",
      "happiness         2           3        8      2         3     0   11   255   \n",
      "boredom          14          25       16     11         6     3    7   865   \n",
      "relief            6           6        5      2        10     0   14   377   \n",
      "anger            31          25       29     16        11     3   21  1455   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             0        0       0      1  \n",
      "enthusiasm          0        1       0      0  \n",
      "neutral             1        4       0      2  \n",
      "worry               5        4       1      1  \n",
      "surprise           13       10       8      8  \n",
      "love                2        2       1      1  \n",
      "fun                 9        6       2      2  \n",
      "hate               21       15      12     11  \n",
      "happiness          10        6       3      2  \n",
      "boredom            21       39      10     16  \n",
      "relief              5        5       3      4  \n",
      "anger              17       34      16     34  \n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Random Forest Bigram Confusion Matrix (100 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness          20           1        0      0         0     1    0     0   \n",
      "enthusiasm       31           1        0      1         1     0    0     0   \n",
      "neutral         122           4        5      6         3     3    2     0   \n",
      "worry           312           3        8     10         6     1    5     0   \n",
      "surprise        846          10       31     25        37     2   47     5   \n",
      "love            245           4        2      3         0     0    1     1   \n",
      "fun             581           9       18     15        21     0  104     3   \n",
      "hate           1542          24       24     22        24     5   27     5   \n",
      "happiness       256           3        9      4         4     3   10     1   \n",
      "boredom         875          23       21     14         6     7    7     0   \n",
      "relief          382           6        5      1         7     2   16     0   \n",
      "anger          1477          23       46     22         7    10   16     2   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             0        0       0      0  \n",
      "enthusiasm          0        0       0      2  \n",
      "neutral             1        3       1      2  \n",
      "worry               6        2       1      1  \n",
      "surprise           17        9      11      2  \n",
      "love                3        4       2      0  \n",
      "fun                 5        4       6      2  \n",
      "hate               24       11      12      8  \n",
      "happiness           7        4       3      1  \n",
      "boredom            20       27      12     21  \n",
      "relief              6        6       3      3  \n",
      "anger              21       27      19     22  \n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Logistic Regression Bigram Confusion Matrix (100 features):\n",
      "            sadness  enthusiasm  neutral  worry  surprise  love  fun  hate  \\\n",
      "sadness           0           1        0      0         0    21    0     0   \n",
      "enthusiasm        0           1        1      1         1    31    0     0   \n",
      "neutral           1           5        5      6         3   125    2     0   \n",
      "worry             4           3        6     13         6   308    4     0   \n",
      "surprise          9          11       33     28        36   841   48     0   \n",
      "love              6           5        1      3         0   242    1     0   \n",
      "fun               6           9       17     14        24   577  102     0   \n",
      "hate             16          29       27     23        22  1534   26     0   \n",
      "happiness         2           3        8      4         2   257   10     0   \n",
      "boredom          14          25       18     17         5   871    5     0   \n",
      "relief            6           6        6      3         9   379   14     0   \n",
      "anger            30          25       41     22         9  1467   16     1   \n",
      "\n",
      "            happiness  boredom  relief  anger  \n",
      "sadness             0        0       0      0  \n",
      "enthusiasm          0        1       0      0  \n",
      "neutral             2        2       1      0  \n",
      "worry               5        3       2      1  \n",
      "surprise           13       10      11      2  \n",
      "love                2        2       2      1  \n",
      "fun                 9        5       3      2  \n",
      "hate               21       12      15      3  \n",
      "happiness          11        4       4      0  \n",
      "boredom            21       34      12     11  \n",
      "relief              5        5       3      1  \n",
      "anger              19       31      16     15  \n",
      "\n",
      "Bigram Performance Comparison:\n",
      "   Features                Model  Precision  Recall     F1\n",
      "0      1000                  SVM      0.275   0.234  0.180\n",
      "1      1000        Random Forest      0.249   0.094  0.106\n",
      "2      1000  Logistic Regression      0.290   0.227  0.169\n",
      "3       500                  SVM      0.276   0.233  0.167\n",
      "4       500        Random Forest      0.253   0.082  0.089\n",
      "5       500  Logistic Regression      0.288   0.226  0.157\n",
      "6       100                  SVM      0.280   0.227  0.133\n",
      "7       100        Random Forest      0.271   0.031  0.046\n",
      "8       100  Logistic Regression      0.229   0.059  0.048\n"
     ]
    }
   ],
   "source": [
    "#question 9\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Configure parameters\n",
    "FEATURE_THRESHOLDS = [1000, 500, 100]\n",
    "MODELS = {\n",
    "    'SVM': LinearSVC(class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced')\n",
    "}\n",
    "# nltk_stopwords = set(stopwords.words('english'))\n",
    "nltk_stopwords = list(stopwords.words('english'))  # Convert to list\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, feature_size):\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            stop_words=nltk_stopwords,\n",
    "            max_features=feature_size,\n",
    "            ngram_range=(2, 2),  # Bigrams only\n",
    "            min_df=3,\n",
    "            max_df=0.9\n",
    "        )),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'precision': report['weighted avg']['precision'],\n",
    "        'recall': report['weighted avg']['recall'],\n",
    "        'f1': report['weighted avg']['f1-score']\n",
    "    }\n",
    "\n",
    "# Main evaluation loop\n",
    "results = {}\n",
    "for feature_size in FEATURE_THRESHOLDS:\n",
    "    print(f\"\\n{'='*40}\\nEvaluating with {feature_size} bigram features\\n{'='*40}\")\n",
    "    feature_results = {}\n",
    "    \n",
    "    for model_name, model in MODELS.items():\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        result = evaluate_model(model, X_train, X_test, y_train, y_test, feature_size)\n",
    "        \n",
    "        # Store results\n",
    "        feature_results[model_name] = {\n",
    "            'metrics': (result['precision'], result['recall'], result['f1']),\n",
    "            'confusion_matrix': result['confusion_matrix']\n",
    "        }\n",
    "        \n",
    "        # Print confusion matrix\n",
    "        print(f\"\\n{model_name} Bigram Confusion Matrix ({feature_size} features):\")\n",
    "        print(pd.DataFrame(result['confusion_matrix'], \n",
    "                         index=df['sentiment'].unique(), \n",
    "                         columns=df['sentiment'].unique()))\n",
    "    \n",
    "    results[feature_size] = feature_results\n",
    "\n",
    "# Compile comparison table\n",
    "comparison = []\n",
    "for feature_size in FEATURE_THRESHOLDS:\n",
    "    for model in MODELS.keys():\n",
    "        prec, rec, f1 = results[feature_size][model]['metrics']\n",
    "        comparison.append({\n",
    "            'Features': feature_size,\n",
    "            'Model': model,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1': f1\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(\"\\nBigram Performance Comparison:\")\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:128: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:128: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\gdemil24\\AppData\\Local\\Temp\\ipykernel_23988\\2498402193.py:128: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  texts = df['content'].str.lower().str.replace('[^a-z0-9\\s]', '').values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test Acc = 0.0051\n",
      "Epoch 2: Test Acc = 0.2066\n",
      "Epoch 3: Test Acc = 0.2195\n",
      "Epoch 4: Test Acc = 0.0028\n",
      "Epoch 5: Test Acc = 0.2287\n",
      "Epoch 6: Test Acc = 0.1002\n",
      "Epoch 7: Test Acc = 0.1299\n",
      "Epoch 8: Test Acc = 0.1594\n",
      "Epoch 9: Test Acc = 0.2821\n",
      "Epoch 10: Test Acc = 0.1013\n",
      "Epoch 11: Test Acc = 0.0126\n",
      "Epoch 12: Test Acc = 0.2018\n",
      "Epoch 13: Test Acc = 0.1408\n",
      "Epoch 14: Test Acc = 0.2796\n",
      "Epoch 15: Test Acc = 0.2517\n",
      "Epoch 16: Test Acc = 0.2463\n",
      "Epoch 17: Test Acc = 0.2597\n",
      "Epoch 18: Test Acc = 0.2148\n",
      "Epoch 19: Test Acc = 0.1332\n",
      "Epoch 20: Test Acc = 0.0763\n",
      "Epoch 21: Test Acc = 0.2277\n",
      "Epoch 22: Test Acc = 0.0063\n",
      "Epoch 23: Test Acc = 0.0258\n",
      "Epoch 24: Test Acc = 0.1832\n",
      "Epoch 25: Test Acc = 0.0051\n",
      "Epoch 26: Test Acc = 0.1652\n",
      "Epoch 27: Test Acc = 0.0110\n",
      "Epoch 28: Test Acc = 0.0812\n",
      "Epoch 29: Test Acc = 0.0975\n",
      "Epoch 30: Test Acc = 0.2388\n",
      "Epoch 31: Test Acc = 0.0198\n",
      "Epoch 32: Test Acc = 0.1981\n",
      "Epoch 33: Test Acc = 0.0891\n",
      "Epoch 34: Test Acc = 0.1301\n",
      "Epoch 35: Test Acc = 0.0138\n",
      "Epoch 36: Test Acc = 0.0994\n",
      "Epoch 37: Test Acc = 0.1496\n",
      "Epoch 38: Test Acc = 0.2406\n",
      "Epoch 39: Test Acc = 0.0713\n",
      "Epoch 40: Test Acc = 0.1542\n",
      "Epoch 41: Test Acc = 0.0407\n",
      "Epoch 42: Test Acc = 0.1377\n",
      "Epoch 43: Test Acc = 0.0324\n",
      "Epoch 44: Test Acc = 0.0304\n",
      "Epoch 45: Test Acc = 0.0343\n",
      "Epoch 46: Test Acc = 0.0343\n",
      "Epoch 47: Test Acc = 0.0545\n",
      "Epoch 48: Test Acc = 0.0734\n",
      "Epoch 49: Test Acc = 0.1777\n",
      "Epoch 50: Test Acc = 0.1085\n",
      "Epoch 51: Test Acc = 0.1555\n",
      "Epoch 52: Test Acc = 0.0359\n",
      "Epoch 53: Test Acc = 0.0665\n",
      "Epoch 54: Test Acc = 0.0200\n",
      "Epoch 55: Test Acc = 0.1324\n",
      "Epoch 56: Test Acc = 0.1449\n",
      "Epoch 57: Test Acc = 0.0343\n",
      "Epoch 58: Test Acc = 0.1057\n",
      "Epoch 59: Test Acc = 0.1857\n",
      "Epoch 60: Test Acc = 0.0333\n",
      "Epoch 61: Test Acc = 0.0387\n",
      "Epoch 62: Test Acc = 0.0194\n",
      "Epoch 63: Test Acc = 0.1431\n",
      "Epoch 64: Test Acc = 0.2059\n",
      "Epoch 65: Test Acc = 0.1142\n",
      "Epoch 66: Test Acc = 0.0475\n",
      "Epoch 67: Test Acc = 0.0389\n",
      "Epoch 68: Test Acc = 0.1528\n",
      "Epoch 69: Test Acc = 0.1214\n",
      "Epoch 70: Test Acc = 0.1024\n",
      "Epoch 71: Test Acc = 0.1013\n",
      "Epoch 72: Test Acc = 0.0462\n",
      "Epoch 73: Test Acc = 0.1728\n",
      "Epoch 74: Test Acc = 0.1945\n",
      "Epoch 75: Test Acc = 0.0771\n",
      "Epoch 76: Test Acc = 0.1105\n",
      "Epoch 77: Test Acc = 0.0391\n",
      "Epoch 78: Test Acc = 0.0456\n",
      "Epoch 79: Test Acc = 0.2184\n",
      "Epoch 80: Test Acc = 0.1154\n",
      "Epoch 81: Test Acc = 0.0909\n",
      "Epoch 82: Test Acc = 0.0537\n",
      "Epoch 83: Test Acc = 0.0447\n",
      "Epoch 84: Test Acc = 0.1976\n",
      "Epoch 85: Test Acc = 0.1138\n",
      "Epoch 86: Test Acc = 0.0373\n",
      "Epoch 87: Test Acc = 0.1257\n",
      "Epoch 88: Test Acc = 0.0730\n",
      "Epoch 89: Test Acc = 0.0071\n",
      "Epoch 90: Test Acc = 0.1303\n",
      "Epoch 91: Test Acc = 0.0879\n",
      "Epoch 92: Test Acc = 0.0415\n",
      "Epoch 93: Test Acc = 0.1267\n",
      "Epoch 94: Test Acc = 0.0860\n",
      "Epoch 95: Test Acc = 0.0221\n",
      "Epoch 96: Test Acc = 0.0227\n",
      "Epoch 97: Test Acc = 0.0354\n",
      "Epoch 98: Test Acc = 0.0129\n",
      "Epoch 99: Test Acc = 0.0743\n",
      "Epoch 100: Test Acc = 0.1085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdemil24\\AppData\\Local\\Temp\\ipykernel_23988\\2498402193.py:164: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00        22\n",
      "     boredom       0.00      0.00      0.00        36\n",
      "  enthusiasm       0.03      0.01      0.01       152\n",
      "         fun       0.11      0.02      0.04       355\n",
      "   happiness       0.26      0.31      0.28      1042\n",
      "        hate       0.20      0.14      0.16       265\n",
      "        love       0.32      0.41      0.36       768\n",
      "     neutral       0.33      0.44      0.38      1728\n",
      "      relief       0.11      0.02      0.03       305\n",
      "     sadness       0.24      0.33      0.28      1033\n",
      "    surprise       0.11      0.06      0.08       437\n",
      "       worry       0.30      0.23      0.26      1692\n",
      "\n",
      "    accuracy                           0.28      7835\n",
      "   macro avg       0.17      0.16      0.16      7835\n",
      "weighted avg       0.26      0.28      0.26      7835\n",
      "\n",
      "\n",
      "Model Comparison:\n",
      "                 Model  Accuracy  F1-Score\n",
      "0         SVM (TF-IDF)     0.685     0.683\n",
      "1  Logistic Regression     0.676     0.674\n",
      "2                  CNN     0.282     0.262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\gdemil24\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#question 10\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Advanced CNN Architecture\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, filter_sizes, num_filters):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Parallel convolutional layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n, (f, embed_dim)) for (n, f) in zip(num_filters, filter_sizes)\n",
    "        ])\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(sum(num_filters), sum(num_filters))\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.bn = nn.BatchNorm1d(sum(num_filters))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "        x = x.unsqueeze(1)     # [batch_size, 1, seq_len, embed_dim]\n",
    "        \n",
    "        # Apply multiple conv filters\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            conv_out = F.relu(conv(x)).squeeze(3)\n",
    "            pooled_out = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)\n",
    "            conv_outputs.append(pooled_out)\n",
    "        \n",
    "        # Combine features\n",
    "        x = torch.cat(conv_outputs, 1)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = torch.sigmoid(self.attention(x))\n",
    "        x = x * attention_weights\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Dataset Class\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenization and padding\n",
    "        tokens = self.tokenizer(text)[:self.max_len]\n",
    "        padded = tokens + [0]*(self.max_len - len(tokens))\n",
    "        \n",
    "        # Explicitly set dtype for labels\n",
    "        return torch.tensor(padded), torch.tensor(label, dtype=torch.long)  # Critical fix\n",
    "\n",
    "# Training Setup\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Class weights for imbalance\n",
    "    class_counts = Counter(y_train)\n",
    "    weights = 1. / torch.Tensor([class_counts[c] for c in range(len(class_counts))])\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "    \n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_acc = evaluate_model(model, test_loader)\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Test Acc = {test_acc:.4f}')\n",
    "    \n",
    "    return best_acc\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv('text_emotion.csv').dropna(subset=['content', 'sentiment'])\n",
    "    df = df[df['sentiment'] != 'empty']\n",
    "    texts = df['content'].str.lower().str.replace('[^a-z0-9\\s]', '').values\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(df['sentiment'])\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Tokenization\n",
    "    tokenizer = lambda x: [hash(word) % 10000 for word in x.split()]  # Simple hash tokenizer\n",
    "    max_len = 100\n",
    "    vocab_size = 10000\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset = EmotionDataset(X_train, y_train, tokenizer, max_len)\n",
    "    test_dataset = EmotionDataset(X_test, y_test, tokenizer, max_len)\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Model parameters\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TextCNN(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=128,\n",
    "        num_classes=len(le.classes_),\n",
    "        filter_sizes=[3, 5, 7],\n",
    "        num_filters=[100, 100, 100]\n",
    "    ).to(device)\n",
    "    \n",
    "    # Training\n",
    "    best_acc = train_model(model, train_loader, test_loader, num_epochs=100)\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            y_pred.extend(torch.argmax(outputs, 1).cpu().numpy())\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Comparison with previous models\n",
    "    comparison = pd.DataFrame({\n",
    "        'Model': ['SVM (TF-IDF)', 'Logistic Regression', 'CNN'],\n",
    "        'Accuracy': [0.685, 0.676, best_acc],\n",
    "        'F1-Score': [0.683, 0.674, classification_report(y_test, y_pred, output_dict=True)['weighted avg']['f1-score']]\n",
    "    })\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(comparison.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
